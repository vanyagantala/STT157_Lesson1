<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Review of Related Literature | Sentiment Analysis: A Machine Learning Approach</title>
  <meta name="description" content="2 Review of Related Literature | Sentiment Analysis: A Machine Learning Approach" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Review of Related Literature | Sentiment Analysis: A Machine Learning Approach" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Review of Related Literature | Sentiment Analysis: A Machine Learning Approach" />
  
  
  

<meta name="author" content="Vanya Ruth Gantala" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="methodology.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html"><i class="fa fa-check"></i><b>2</b> Review of Related Literature</a>
<ul>
<li class="chapter" data-level="2.1" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#sentiment-analysis"><i class="fa fa-check"></i><b>2.1</b> Sentiment Analysis</a></li>
<li class="chapter" data-level="2.2" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#pre-processing"><i class="fa fa-check"></i><b>2.2</b> Pre-processing</a></li>
<li class="chapter" data-level="2.3" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#sentiment-analysis-approach"><i class="fa fa-check"></i><b>2.3</b> Sentiment Analysis Approach</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#machine-learning-approach"><i class="fa fa-check"></i><b>2.3.1</b> Machine Learning Approach</a></li>
<li class="chapter" data-level="2.3.2" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#lexicon-based-approach"><i class="fa fa-check"></i><b>2.3.2</b> Lexicon-based Approach</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#application"><i class="fa fa-check"></i><b>2.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="methodology.html"><a href="methodology.html"><i class="fa fa-check"></i><b>3</b> Methodology</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Sentiment Analysis: A Machine Learning Approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="review-of-related-literature" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Review of Related Literature</h1>
<p>The literature review discusses the various studies conducted by researchers on sentiment analysis and some additional information about the analysis.</p>
<div id="sentiment-analysis" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Sentiment Analysis</h2>
<p>Texts are unstructured. Thus, there is a need to undergo analysis to gather data not visible by the text itself. Lee et al. (2014) concluded that text has predictive power for stock price movement. Their conclusion is evidence that one cannot just ignore the importance of text in data analysis.</p>
<p>Text analytics and sentiment analysis are methods to extract usable data from texts; however, they differ. As stated by Bauer, Bicquelet, and Suerdem (2014), text analytics aims to analyze and discover the meanings behind a text from an expert point of view. This method includes the text structure not limited to the grammar and the relationship among the words for the analysis. On the other hand, according to Gupta et al. (2017), sentiment analysis is a technique that derives opinions from expressions and retrieves sentiment that it
mimics. Sentiment analysis determines if a text is positive, negative, or neutral.</p>
<p>Medhat, Hassan, &amp; Korashy (2014) classified sentiment analysis into three levels:
document, sentence, and aspect. Document-level sentiment analysis takes the whole
document to identify what expression is radiated, positive, negative, or neutral opinion. Sentence level analyzes each sentence of a document by first identifying whether the sentence is subjective or objective. Each sentence will then be categorized according to its opinion, positive, negative, or neutral sentiment. However, document level and sentence level sentiment analyses do not provide vital details on all aspects of the entity. Hence, there is aspect-level sentiment analysis. This level aims to determine sentiments with regards to an opinion’s specific aspects.</p>
</div>
<div id="pre-processing" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Pre-processing</h2>
<p>Pre-processing of the data prepares the data before it undergoes analysis since texts usually have lots of noise. Krouska, Troussas, and Virvou (2016) explored various pre- processing options on three different data sets. A total of five pre-processing techniques were covered in their study and were employed in the WEKA data mining package. First is the weighting scheme using TF-IDF (term frequency-inverse document frequency). TF-IDF provides a numerical statistic that shows how a word is vital to a document in a corpus. The following technique is stemming, the process of removing suffixes of the word. They utilized the library, Snowball stemmer, which is considered as the most liked and standard approach. The third technique is stop-words removal using Rainbow list. In this step, words that are not necessary for the text classification are eliminated. Tokenization is the fourth technique outlined in this study, which results in a word vector bag-of-words. They proposed NGramTokenizer to compare words unigram, bigram, and 1-to-3-gram. Lastly, feature selection was performed. This last technique decreases the number of attributes to achieve much higher accuracy. There will be limitations for overfitting, improvement of accuracy, and lessening training time by making feature selection. Through WEKA, the AttributeSelection filter is provided to pick an attribute evaluation method and a search strategy. Three options are examined in this study: no filter, InfoGainAttributeEval, which assesses the worth of an attribute by measuring information accumulation for the class, and ClassifierAttributeEval, which also assesses the worth of an attribute by employing a user- specified classifier. Results show that 1-to-3-gram performs better than other representations, with almost 90% accuracy for some classifiers and data sets. Moreover, in some classifiers, attribute selection gives higher accuracy compared to selecting all attributes.</p>
</div>
<div id="sentiment-analysis-approach" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Sentiment Analysis Approach</h2>
<p>Numerous studies use two types of sentiment analysis approaches the machine learning approach and the lexicon-based approach</p>
<div id="machine-learning-approach" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Machine Learning Approach</h3>
<p>The machine learning approach works by taking a training data set before a final classifier and applying the same data set. A comparative study by Devika, Sunitha, and Ganesh (2016) revealed two advantages with this approach: the unnecessary need for a dictionary and the high accuracy of classification. However, this approach has a disadvantage; the classifier may not work with other areas besides the initially trained area. There are two classifications for this approach, supervised and unsupervised learning. The supervised learning method depends on identified training documents. Under supervised learning, there are various classifiers: decision tree, linear, rule-based, and probabilistic. The unsupervised learning method, on the other hand, overcame the difficulties of creating identified training documents.</p>
<p>Four classifiers under supervised learning were discussed in a sentiment analysis algorithms and applications survey by Medhat et al. (2014)</p>
<div id="decision-tree-classifiers" class="section level4" number="2.3.1.1">
<h4><span class="header-section-number">2.3.1.1</span> Decision Tree Classifiers</h4>
<p>Decision tree classifiers divide data by condition on the attribute value. This condition is the presence or absence of a word or set of words. The process proceeds until the leaf nodes contain the minimum amount of records which will then be used for classification. Some other kinds of conditions can be used for the partitioning of the documents later on. That is, the documents are checked for possible similarities to correlate a set of words. Three splits are exposed in their study: single attribute split, which functions when there is a presence or absence of word or words at a specific node in the tree, similarity-based multi- attribute split, splits by taking the similarities of documents to a predetermined word cluster, and discriminant-based multi-attribute split, the split is performed using discriminants such as Fisher discriminate.</p>
</div>
<div id="linear-classifiers" class="section level4" number="2.3.1.2">
<h4><span class="header-section-number">2.3.1.2</span> Linear Classifiers</h4>
<p>Their study reviews two of the most popular linear classifiers, support vector machines, and neural networks.</p>
<div id="support-vector-machines-classifiers" class="section level5" number="2.3.1.2.1">
<h5><span class="header-section-number">2.3.1.2.1</span> Support Vector Machines Classifiers</h5>
<p>A text has a sparse nature where few features are irrelevant but can be correlated somehow, which is why a support vector machine classifier is ideal for text data. This classifier takes a large amount of training set, which is a disadvantage. Accordingly, SVM has the advantage of having high-dimensional input space, some extraneous features, and scattered document vectors (Devika et al., 2016). In addition, Zainuddin and Selamat (2014) used SVM in their study to classify whether texts are positive or negative from benchmark data sets with a ratio of 70:30 for training and testing set, respectively. Choosing the features based on their chi-squared statistics value reduced the dimensionality and noise in the text, resulting in the classifier’s high accuracy.</p>
</div>
<div id="neural-network" class="section level5" number="2.3.1.2.2">
<h5><span class="header-section-number">2.3.1.2.2</span> Neural Network</h5>
<p>Neurons are the basic unit in the neural network, and the inputs to these neurons are word frequencies in a document. Several studies use neural network methods for their analysis, such as the convolutional neural network. Ouyang et al. (2015) conducted sentiment analysis on movie reviews using a convolutional neural network, a deep learning model and is easier to train than other deep learning methods. With the combination of the word2vec tool, a neural net that processes texts, the convolutional neural network can outperform some simple classification algorithms.</p>
</div>
</div>
<div id="rule-based-classifiers" class="section level4" number="2.3.1.3">
<h4><span class="header-section-number">2.3.1.3</span> Rule-based Classifiers</h4>
<p>The data space in a rule-based classifier is modeled with a collection of rules. This approach is used by defining diverse rules for getting the opinion, and it works by testing each token from each sentence in a document (Devika et al., 2016). Bidulya and Brunova (2016) applied a total of nine rules in a specific order in their study. They used lexical and syntactic structures with a positive and negative lexicon in 200 customer reviews of a Russian bank’s service quality. The study uses the Naive-Bayes classifier compared to the rule-based classifier. As a result, the latter exceeded the Naive-Bayes classifier.</p>
</div>
<div id="probabilistic-classifiers" class="section level4" number="2.3.1.4">
<h4><span class="header-section-number">2.3.1.4</span> Probabilistic Classifiers</h4>
<p>Mixture models for classification are used in this type of classifier. A mixture model assumes that every class is a part of the mixture. This mixture component provides a likelihood of sampling a particular term. Probabilistic classifiers are also recognized as generative classifiers.</p>
<div id="naive-bayes-method" class="section level5" number="2.3.1.4.1">
<h5><span class="header-section-number">2.3.1.4.1</span> Naive Bayes Method</h5>
<p>The simplest and the most commonly used classifier is Naive Bayes. This classifier uses the Bayes Theorem to predict the probability that a given feature set goes to an appropriate label. Naive Bayes classifier assumes the independence of the features. Several studies use Naive Bayes as a classifier because of its speed, including the study of Goel, Gautam, and Kumar (2016). They trained 1.6 million tweets and tested 100 most recent tweets wherein the system returns an accuracy of 58.40%. They added that this accuracy could be improved using sentiwordnet, a lexical database for the English language, and Naive Bayes as the classifier.</p>
</div>
<div id="bayesian-network" class="section level5" number="2.3.1.4.2">
<h5><span class="header-section-number">2.3.1.4.2</span> Bayesian Network</h5>
<p>A Bayesian network is a full model for the variables or large sets of words and their relationships. This classifier is a frequently used model, especially in text mining due to its expensive nature.</p>
</div>
<div id="maximum-entropy-classifier" class="section level5" number="2.3.1.4.3">
<h5><span class="header-section-number">2.3.1.4.3</span> Maximum Entropy Classifier</h5>
<p>Also identified as a conditional exponential classifier, this classifier can handle extensive data without assuming the features’ independence. Liu and Meng (2014) conducted an aspect-based sentiment analysis with several subtasks, aspect term extraction, aspect category detection, aspect term polarity, and aspect category polarity. They used a maximum entropy classifier for these subtasks, except aspect term extraction, and returned differing performances. Applying boosting method on the classifier gave high accuracy for the subtask of aspect category detection. Other subtasks also gained average accuracies with maximum entropy.</p>
<p>Gupta et al. (2017) used machine learning algorithms for Twitter sentiment analysis with Python as the programming language. Their data have undergone various pre-processing steps to make it more practical for machine learning. These pre-processing steps include removal of retweets, conversion of the upper case to lower case, stopword removal, removal of twitter feature, stemming, elimination of unique characters and digits, dictionary creation to remove unimportant words and punctuation marks, expansion of slangs and abbreviations, spelling correction, generating a dictionary for essential words and emoticons, and part of speech tagging. They used unigram features, N-gram features, and external lexicon for feature extraction. Various sentiment classifiers are used and compared in this study: Bayesian Logistic, Naive Bayes, Support Vector Machine, Artificial Neural Network, Case Base Reasoning, Maximum Entropy Classifier, and Ensemble Classifier. The collected data set was divided into two sets: training and testing. They added that it is upon the user to choose the number of classes for classification but bear in mind that as the number of classes increases, the performance of classifiers decreases. Upon comparing each classifier’s accuracy, maximum entropy and ensemble classifier achieved the highest accuracy.</p>
</div>
</div>
</div>
<div id="lexicon-based-approach" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Lexicon-based Approach</h3>
<p>The lexicon-based approach is unsupervised learning with the advantage of not requiring labeled data and a learning procedure and the disadvantage of needing vital linguistic resources (Devika et al., 2016). This approach has varying techniques, such as dictionary-based and corpus-based methods. Taj, Shaikh, and Meghji (2019) briefly mentioned these methods. The dictionary-based method uses a lexicon dictionary to sort positive opinion and negative opinion words. In contrast, the corpus-based method uses an extensive collection of words. Other opinion words are detected within the context based on syntactic patterns.</p>
<p>Taj et al. (2019) employed lexicon-based sentiment analysis on the BBC news data set. Their study seeks to identify positive, negative, or neutral opinions from the latest articles. A dictionary-based approach was used in this study, starting with collecting data, text pre- processing, calculation of sentiment polarity, calculation of sentiment scores, and, lastly, sentiment results. The whole pre-processing steps were carried out using the Rapid miner tool, which provides numerous sets for pre-processing. A statistical tool known as TF-IDF was used to rank words according to their occurrence in the news article. Afterward, to determine the polarity of words and assign sentiment scores, they used the WordNet lexical dictionary. After the total sentiment score calculation using the extract sentiment operator, news articles were classified into positive, negative, and neutral.</p>
<p>Kumar and Sebastian (2012) introduced a hybrid approach involving both dictionary- based and corpus-based methods. A series of pre-processing steps were implemented to their Twitter data set. A dictionary-based method was used to find semantic orientations of verbs and adverbs to heed the importance of some adverbs, such as not, with the help of the dictionary WordNet. Since adjectives tend to be domain-specific, a corpus-based method was used. Their scoring method has two groups: adjective group with both adverb and adjective, and verb group with both adverb and verb. In order to calculate the overall sentiment of the tweet, they took the mean strength of all opinion indicators like emoticons, exclamation marks, and more. In addition, they proposed a formula to achieve the Twitter sentiment score and employed it in some sample tweets. Results show that their proposed system has the qualities of finding the semantic orientation of tweets.</p>
<p>Three dictionaries were compared in Roul’s (2021) comparative discussion: AFFIN Lexicon, the most popular and simplest lexicons, SentiWordNet, a lexical source for sentiment analysis, and VADER or Valence Aware Dictionary and Sentiment Reasoner, a lexicon specifically made for social media sentiments. These dictionaries return almost equivalent accuracy with 72% accuracy for the AFFIN lexical model, which is the highest. Furthermore, supervised learning using the models SVM and Logistic Regression was compared using the Bag of Words Model and TF-IDF to extract features. Using a bag of words model, the logistic regression model returned the highest accuracy of all the models, including lexical models.</p>
</div>
</div>
<div id="application" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Application</h2>
<p>Feldman (2013) discussed several applications of sentiment analysis in his study. The most common target for sentiment analysis is consumer reviews on products and services. Social media sites such as Twitter and Facebook are good data sources considering some businesses monitor their reputation on the said sites. In addition, politicians can make use of sentiment analysis. Campaign managers can track voters’ feelings about different issues, statements, or speeches regarding the candidates. The financial or stock market can also utilize sentiment analysis since sentiments, positive or negative, significantly affect stock price movement.</p>
<!-- Lorem ipsum dolor sit amet, lobortis non senectus purus quis in at sed at, condimentum. Cras potenti nunc ad quam class nulla montes purus, est suscipit. Ac, nullam consectetur integer varius aptent. Aliquet ac, senectus ante sit a. Venenatis efficitur purus ut curae neque proin iaculis. Elementum volutpat nec. Consectetur erat dapibus non faucibus ac mauris, vel eleifend. Porttitor lacinia blandit tempor iaculis nec torquent fusce fermentum. -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methodology.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
